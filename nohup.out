[INFO][AppLauncher]: Loading experience file: /home/dl-box/codes/anhar/isaaclab_rl_experiments/IsaacLab/apps/isaaclab.python.headless.kit
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.a1.flat_env_cfg:UnitreeA1FlatEnvCfg
[INFO]: Parsing configuration from: isaaclab_tasks.manager_based.locomotion.velocity.config.a1.agents.rsl_rl_ppo_cfg:UnitreeA1FlatPPORunnerCfg
[Warning] [simulation_app.simulation_app] Modules: ['omni.kit_app'] were loaded before SimulationApp was started and might not be loaded correctly.
[Warning] [simulation_app.simulation_app] Please check to make sure no extra omniverse or pxr modules are imported before the call to SimulationApp(...)
Loading user config located at: '/home/dl-box/codes/anhar/isaaclab_rl_experiments/isaaclab_env/lib/python3.10/site-packages/omni/data/Kit/Isaac-Sim/4.5/user.config.json'
[Info] [carb] Logging to file: /home/dl-box/codes/anhar/isaaclab_rl_experiments/isaaclab_env/lib/python3.10/site-packages/omni/logs/Kit/Isaac-Sim/4.5/kit_20250409_175933.log
2025-04-09 08:59:33 [0ms] [Warning] [omni.kit.app.plugin] No crash reporter present, dumps uploading isn't available.
2025-04-09 08:59:34 [141ms] [Warning] [omni.usd_config.extension] Enable omni.materialx.libs extension to use MaterialX
2025-04-09 08:59:34 [548ms] [Warning] [omni.datastore] OmniHub is inaccessible
2025-04-09 08:59:34 [659ms] [Warning] [omni.isaac.dynamic_control] omni.isaac.dynamic_control is deprecated as of Isaac Sim 4.5. No action is needed from end-users.

|---------------------------------------------------------------------------------------------|
| Driver Version: 555.52.04     | Graphics API: Vulkan
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|     |                                  |        |     |            | Bus-ID    |            |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA RTX 6000 Ada Generation   | Yes: 0 |     | 49140   MB | 10de      | 0          |
|     |                                  |        |     |            | 26b1      | d370752b.. |
|     |                                  |        |     |            | 16        |            |
|=============================================================================================|
| OS: 22.04.4 LTS (Jammy Jellyfish) ubuntu, Version: 22.04.4, Kernel: 5.15.0-117-generic
| XServer Vendor: The X.Org Foundation, XServer Version: 12101006 (1.21.1.6)
| Processor: Intel(R) Xeon(R) w7-3465X
| Cores: 28 | Logical Cores: 56
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 128308 | Free Memory: 113332
| Total Page/Swap (MB): 16383 | Free Page/Swap: 16383
|---------------------------------------------------------------------------------------------|
2025-04-09 08:59:40 [6,784ms] [Warning] [isaaclab.terrains.terrain_importer] Visual material specified for ground plane but no diffuse color found. Using default color: (0.0, 0.0, 0.0)
2025-04-09 08:59:41 [7,978ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPreviewSurface.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-04-09 08:59:41 [7,978ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdUVTexture.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-04-09 08:59:41 [7,978ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_float.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-04-09 08:59:41 [7,978ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_float2.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-04-09 08:59:41 [7,978ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at lin[INFO] Logging experiment in directory: /home/dl-box/codes/anhar/isaaclab_rl_experiments/logs/rsl_rl/unitree_a1_flat
Exact experiment name requested from command line: 2025-04-09_17-59-40
Setting seed: 42
[INFO]: Base environment:
	Environment device    : cuda:0
	Environment seed      : 42
	Physics step-size     : 0.005
	Rendering step-size   : 0.02
	Environment step-size : 0.02
[INFO]: Time taken for scene creation : 1.041422 seconds
[INFO]: Scene manager:  <class InteractiveScene>
	Number of environments: 100
	Environment spacing   : 2.5
	Source prim name      : /World/envs/env_0
	Global prim paths     : ['/World/ground']
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
[INFO]: Time taken for simulation start : 0.702900 seconds
[INFO] Command Manager:  <CommandManager> contains 1 active terms.
+------------------------------------------------+
|              Active Command Terms              |
+-------+---------------+------------------------+
| Index | Name          |          Type          |
+-------+---------------+------------------------+
|   0   | base_velocity | UniformVelocityCommand |
+-------+---------------+------------------------+

[INFO] Event Manager:  <EventManager> contains 2 active terms.
+--------------------------------------+
| Active Event Terms in Mode: 'startup' |
+----------+---------------------------+
|  Index   | Name                      |
+----------+---------------------------+
|    0     | physics_material          |
|    1     | add_base_mass             |
+----------+---------------------------+
+---------------------------------------+
|  Active Event Terms in Mode: 'reset'  |
+--------+------------------------------+
| Index  | Name                         |
+--------+------------------------------+
|   0    | base_external_force_torque   |
|   1    | reset_base                   |
|   2    | reset_robot_joints           |
+--------+------------------------------+

[INFO] Recorder Manager:  <RecorderManager> contains 0 active terms.
+---------------------+
| Active Recorder Terms |
+-----------+---------+
|   Index   | Name    |
+-----------+---------+
+-----------+---------+

[INFO] Action Manager:  <ActionManager> contains 1 active terms.
+------------------------------------+
|  Active Action Terms (shape: 12)   |
+--------+-------------+-------------+
| Index  | Name        |   Dimension |
+--------+-------------+-------------+
|   0    | joint_pos   |          12 |
+--------+-------------+-------------+

[INFO] Observation Manager: <ObservationManager> contains 1 groups.
+---------------------------------------------------------+
| Active Observation Terms in Group: 'policy' (shape: (48,)) |
+-----------+---------------------------------+-----------+
|   Index   | Name                            |   Shape   |
+-----------+---------------------------------+-----------+
|     0     | base_lin_vel                    |    (3,)   |
|     1     | base_ang_vel                    |    (3,)   |
|     2     | projected_gravity               |    (3,)   |
|     3     | velocity_commands               |    (3,)   |
|     4     | joint_pos                       |   (12,)   |
|     5     | joint_vel                       |   (12,)   |
|     6     | actions                         |   (12,)   |
+-----------+---------------------------------+-----------+

[INFO] Termination Manager:  <TerminationManager> contains 2 active terms.
+---------------------------------+
|     Active Termination Terms    |
+-------+--------------+----------+
| Index | Name         | Time Out |
+-------+--------------+----------+
|   0   | time_out     |   True   |
|   1   | base_contact |  False   |
+-------+--------------+----------+

[INFO] Reward Manager:  <RewardManager> contains 10 active terms.
+-----------------------------------------+
|           Active Reward Terms           |
+-------+----------------------+----------+
| Index | Name                 |   Weight |
+-------+----------------------+----------+
|   0   | track_lin_vel_xy_exp |      1.5 |
|   1   | track_ang_vel_z_exp  |     0.75 |
|   2   | lin_vel_z_l2         |     -2.0 |
|   3   | ang_vel_xy_l2        |    -0.05 |
|   4   | dof_torques_l2       |  -0.0002 |
|   5   | dof_acc_l2           | -2.5e-07 |
|   6   | action_rate_l2       |    -0.01 |
|   7   | feet_air_time        |     0.25 |
|   8   | flat_orientation_l2  |     -2.5 |
|   9   | dof_pos_limits       |      0.0 |
+-------+----------------------+----------+

[INFO] Curriculum Manager:  <CurriculumManager> contains 0 active terms.
+----------------------+
| Active Curriculum Terms |
+-----------+----------+
|   Index   | Name     |
+-----------+----------+
+-----------+----------+

[INFO]: Completed setting up the environment...
Actor MLP: Sequential(
  (0): Linear(in_features=48, out_features=128, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=128, out_features=128, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=128, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=12, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=48, out_features=128, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=128, out_features=128, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=128, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       [1m Learning iteration 0/300 [0m                       

                       Computation: 1167 steps/s (collection: 1.828s, learning 0.228s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.1288
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 17.0223
                       Mean reward: -0.85
               Mean episode length: 16.40
Episode_Reward/track_lin_vel_xy_exp: 0.0025
Episode_Reward/track_ang_vel_z_exp: 0.0023
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0062
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.0029
     Episode_Reward/action_rate_l2: -0.0021
      Episode_Reward/feet_air_time: -0.0003
Episode_Reward/flat_orientation_l2: -0.0005
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0105
Metrics/base_velocity/error_vel_yaw: 0.0106
      Episode_Termination/time_out: 0.4167
  Episode_Termination/base_contact: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2400
                    Iteration time: 2.06s
                        Total time: 2.06s
                               ETA: 616.7s

Storing git diff for 'isaaclab_rl_experiments' in: /home/dl-box/codes/anhar/isaaclab_rl_experiments/logs/rsl_rl/unitree_a1_flat/2025-04-09_17-59-40/git/isaaclab_rl_experiments.diff
################################################################################
                       [1m Learning iteration 1/300 [0m                       

                       Computation: 5541 steps/s (collection: 0.349s, learning 0.085s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0452
               Mean surrogate loss: -0.0167
                 Mean entropy loss: 17.0184
                       Mean reward: -1.02
               Mean episode length: 25.10
Episode_Reward/track_lin_vel_xy_exp: 0.0147
Episode_Reward/track_ang_vel_z_exp: 0.0030
       Episode_Reward/lin_vel_z_l2: -0.0269
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.0028
         Episode_Reward/dof_acc_l2: -0.0086
     Episode_Reward/action_rate_l2: -0.0055
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: -0.0029
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0259
Metrics/base_velocity/error_vel_yaw: 0.0542
      Episode_Termination/time_out: 0.3750
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4800
                    Iteration time: 0.43s
                        Total time: 2.49s
                               ETA: 372.1s

################################################################################
                       [1m Learning iteration 2/300 [0m                       

                       Computation: 5678 steps/s (collection: 0.340s, learning 0.083s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0549
               Mean surrogate loss: -0.0174
                 Mean entropy loss: 17.0011
                       Mean reward: -1.11
               Mean episode length: 29.92
Episode_Reward/track_lin_vel_xy_exp: 0.0585
Episode_Reward/track_ang_vel_z_exp: 0.0076
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.0549
     Episode_Reward/dof_torques_l2: -0.0054
         Episode_Reward/dof_acc_l2: -0.0179
     Episode_Reward/action_rate_l2: -0.0127
      Episode_Reward/feet_air_time: -0.0031
Episode_Reward/flat_orientation_l2: -0.0030
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0309
Metrics/base_velocity/error_vel_yaw: 0.1226
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7200
                    Iteration time: 0.42s
                        Total time: 2.91s
                               ETA: 289.2s

################################################################################
                       [1m Learning iteration 3/300 [0m                       

                       Computation: 5747 steps/s (collection: 0.337s, learning 0.080s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0497
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 16.9828
                       Mean reward: -1.28
               Mean episode length: 42.00
Episode_Reward/track_lin_vel_xy_exp: 0.0750
Episode_Reward/track_ang_vel_z_exp: 0.0109
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0680
     Episode_Reward/dof_torques_l2: -0.0066
         Episode_Reward/dof_acc_l2: -0.0175
     Episode_Reward/action_rate_l2: -0.0158
      Episode_Reward/feet_air_time: -0.0039
Episode_Reward/flat_orientation_l2: -0.0035
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0337
Metrics/base_velocity/error_vel_yaw: 0.1422
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9600
                    Iteration time: 0.42s
                        Total time: 3.33s
                               ETA: 247.2s

################################################################################
                       [1m Learning iteration 4/300 [0m                       

                       Computation: 5529 steps/s (collection: 0.353s, learning 0.081s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0452
               Mean surrogate loss: -0.0201
                 Mean entropy loss: 16.9636
                       Mean reward: -2.11
               Mean episode length: 62.10
Episode_Reward/track_lin_vel_xy_exp: 0.0314
Episode_Reward/track_ang_vel_z_exp: 0.0230
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.0862
     Episode_Reward/dof_torques_l2: -0.0106
         Episode_Reward/dof_acc_l2: -0.0306
     Episode_Reward/action_rate_l2: -0.0260
      Episode_Reward/feet_air_time: -0.0067
Episode_Reward/flat_orientation_l2: -0.0119
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1695
Metrics/base_velocity/error_vel_yaw: 0.1896
      Episode_Termination/time_out: 0.8750
  Episode_Termination/base_contact: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12000
                    Iteration time: 0.43s
                        Total time: 3.76s
                               ETA: 222.8s

################################################################################
                       [1m Learning iteration 5/300 [0m                       

                       Computation: 5697 steps/s (collection: 0.340s, learning 0.081s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0416
               Mean surrogate loss: -0.0168
                 Mean entropy loss: 16.9467
                       Mean reward: -2.43
               Mean episode length: 71.17
Episode_Reward/track_lin_vel_xy_exp: 0.0054
Episode_Reward/track_ang_vel_z_exp: 0.0249
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.0932
     Episode_Reward/dof_torques_l2: -0.0122
         Episode_Reward/dof_acc_l2: -0.0333
     Episode_Reward/action_rate_l2: -0.0294
      Episode_Reward/feet_air_time: -0.0080
Episode_Reward/flat_orientation_l2: -0.0078
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2699
Metrics/base_velocity/error_vel_yaw: 0.2453
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14400
                    Iteration time: 0.42s
                        Total time: 4.18s
                               ETA: 205.7s

################################################################################
                       [1m Learning iteration 6/300 [0m                       

                       Computation: 5589 steps/s (collection: 0.347s, learning 0.082s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0354
               Mean surrogate loss: -0.0203
                 Mean entropy loss: 16.9223
                       Mean reward: -2.84
               Mean episode length: 86.45
Episode_Reward/track_lin_vel_xy_exp: 0.0208
Episode_Reward/track_ang_vel_z_exp: 0.0297
       Episode_Reward/lin_vel_z_l2: -0.0530
      Episode_Reward/ang_vel_xy_l2: -0.1124
     Episode_Reward/dof_torques_l2: -0.0161
         Episode_Reward/dof_acc_l2: -0.0424
     Episode_Reward/action_rate_l2: -0.0374
      Episode_Reward/feet_air_time: -0.0099
Episode_Reward/flat_orientation_l2: -0.0094
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2907
Metrics/base_velocity/error_vel_yaw: 0.3062
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16800
                    Iteration time: 0.43s
                        Total time: 4.61s
                               ETA: 193.8s

################################################################################
                       [1m Learning iteration 7/300 [0m                       

                       Computation: 5681 steps/s (collection: 0.340s, learning 0.082s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0353
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 16.8999
                       Mean reward: -2.93
               Mean episode length: 92.35
Episode_Reward/track_lin_vel_xy_exp: 0.0288
Episode_Reward/track_ang_vel_z_exp: 0.0476
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.1275
     Episode_Reward/dof_torques_l2: -0.0182
         Episode_Reward/dof_acc_l2: -0.0393
     Episode_Reward/action_rate_l2: -0.0446
      Episode_Reward/feet_air_time: -0.0119
Episode_Reward/flat_orientation_l2: -0.0119
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.3027
Metrics/base_velocity/error_vel_yaw: 0.2616
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19200
                    Iteration time: 0.42s
                        Total time: 5.04s
                               ETA: 184.5s

################################################################################
                       [1m Learning iteration 8/300 [0m                       

                       Computation: 5764 steps/s (collection: 0.337s, learning 0.079s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 16.8738
                       Mean reward: -2.93
               Mean episode length: 92.35
Episode_Reward/track_lin_vel_xy_exp: 0.0534
Episode_Reward/track_ang_vel_z_exp: 0.0477
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.1294
     Episode_Reward/dof_torques_l2: -0.0187
         Episode_Reward/dof_acc_l2: -0.0413
     Episode_Reward/action_rate_l2: -0.0470
      Episode_Reward/feet_air_time: -0.0123
Episode_Reward/flat_orientation_l2: -0.0080
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2403
Metrics/base_velocity/error_vel_yaw: 0.2793
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21600
                    Iteration time: 0.42s
                        Total time: 5.45s
                               ETA: 176.9s

################################################################################
                       [1m Learning iteration 9/300 [0m                       

                       Computation: 5574 steps/s (collection: 0.351s, learning 0.079s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0173
                 Mean entropy loss: 16.8500
                       Mean reward: -3.28
               Mean episode length: 107.34
Episode_Reward/track_lin_vel_xy_exp: 0.0135
Episode_Reward/track_ang_vel_z_exp: 0.0484
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.1698
     Episode_Reward/dof_torques_l2: -0.0232
         Episode_Reward/dof_acc_l2: -0.0639
     Episode_Reward/action_rate_l2: -0.0534
      Episode_Reward/feet_air_time: -0.0139
Episode_Reward/flat_orientation_l2: -0.0191
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.4667
Metrics/base_velocity/error_vel_yaw: 0.4262
      Episode_Termination/time_out: 0.8333
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24000
                    Iteration time: 0.43s
                        Total time: 5.88s
                               ETA: 171.2s

################################################################################
                      [1m Learning iteration 10/300 [0m                       

                       Computation: 5468 steps/s (collection: 0.356s, learning 0.083s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0182
                 Mean entropy loss: 16.8330
                       Mean reward: -3.69
               Mean episode length: 125.45
Episode_Reward/track_lin_vel_xy_exp: 0.0311
Episode_Reward/track_ang_vel_z_exp: 0.0523
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1833
     Episode_Reward/dof_torques_l2: -0.0249
         Episode_Reward/dof_acc_l2: -0.0615
     Episode_Reward/action_rate_l2: -0.0587
      Episode_Reward/feet_air_time: -0.0155
Episode_Reward/flat_orientation_l2: -0.0182
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.5123
Metrics/base_velocity/error_vel_yaw: 0.4694
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26400
                    Iteration time: 0.44s
                        Total time: 6.32s
                               ETA: 166.7s

################################################################################
                      [1m Learning iteration 11/300 [0m                       

                       Computation: 5301 steps/s (collection: 0.367s, learning 0.086s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0307
               Mean surrogate loss: -0.0171
                 Mean entropy loss: 16.8214
                       Mean reward: -3.93
               Mean episode length: 144.50
Episode_Reward/track_lin_vel_xy_exp: 0.1241
Episode_Reward/track_ang_vel_z_exp: 0.0606
       Episode_Reward/lin_vel_z_l2: -0.0581
      Episode_Reward/ang_vel_xy_l2: -0.2108
     Episode_Reward/dof_torques_l2: -0.0266
         Episode_Reward/dof_acc_l2: -0.0743
     Episode_Reward/action_rate_l2: -0.0681
      Episode_Reward/feet_air_time: -0.0165
Episode_Reward/flat_orientation_l2: -0.0191
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.3692
Metrics/base_velocity/error_vel_yaw: 0.4911
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28800
                    Iteration time: 0.45s
                        Total time: 6.77s
                               ETA: 163.2s

################################################################################
                      [1m Learning iteration 12/300 [0m                       

                       Computation: 5527 steps/s (collection: 0.350s, learning 0.084s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 16.8114
                       Mean reward: -4.02
               Mean episode length: 150.85
Episode_Reward/track_lin_vel_xy_exp: 0.2367
Episode_Reward/track_ang_vel_z_exp: 0.0447
       Episode_Reward/lin_vel_z_l2: -0.0654
      Episode_Reward/ang_vel_xy_l2: -0.2601
     Episode_Reward/dof_torques_l2: -0.0281
         Episode_Reward/dof_acc_l2: -0.1044
     Episode_Reward/action_rate_l2: -0.0748
      Episode_Reward/feet_air_time: -0.0181
Episode_Reward/flat_orientation_l2: -0.0360
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2466
Metrics/base_velocity/error_vel_yaw: 0.7263
      Episode_Termination/time_out: 0.5833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31200
                    Iteration time: 0.43s
                        Total time: 7.21s
                               ETA: 159.7s

################################################################################
                      [1m Learning iteration 13/300 [0m                       

                       Computation: 5589 steps/s (collection: 0.346s, learning 0.084s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0195
               Mean surrogate loss: -0.0188
                 Mean entropy loss: 16.7969
                       Mean reward: -4.06
               Mean episode length: 154.35
Episode_Reward/track_lin_vel_xy_exp: 0.1830
Episode_Reward/track_ang_vel_z_exp: 0.0611
       Episode_Reward/lin_vel_z_l2: -0.0670
      Episode_Reward/ang_vel_xy_l2: -0.2342
     Episode_Reward/dof_torques_l2: -0.0301
         Episode_Reward/dof_acc_l2: -0.1055
     Episode_Reward/action_rate_l2: -0.0753
      Episode_Reward/feet_air_time: -0.0191
Episode_Reward/flat_orientation_l2: -0.0200
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.3177
Metrics/base_velocity/error_vel_yaw: 0.6496
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33600
                    Iteration time: 0.43s
                        Total time: 7.64s
                               ETA: 156.6s

################################################################################
                      [1m Learning iteration 14/300 [0m                       

                       Computation: 5567 steps/s (collection: 0.348s, learning 0.083s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0210
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 16.7724
                       Mean reward: -4.33
               Mean episode length: 165.46
Episode_Reward/track_lin_vel_xy_exp: 0.0490
Episode_Reward/track_ang_vel_z_exp: 0.0707
       Episode_Reward/lin_vel_z_l2: -0.0650
      Episode_Reward/ang_vel_xy_l2: -0.2518
     Episode_Reward/dof_torques_l2: -0.0332
         Episode_Reward/dof_acc_l2: -0.0982
     Episode_Reward/action_rate_l2: -0.0841
      Episode_Reward/feet_air_time: -0.0203
Episode_Reward/flat_orientation_l2: -0.0232
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.6867
Metrics/base_velocity/error_vel_yaw: 0.6693
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36000
                    Iteration time: 0.43s
                        Total time: 8.07s
                               ETA: 153.9s

################################################################################
                      [1m Learning iteration 15/300 [0m                       

                       Computation: 5575 steps/s (collection: 0.345s, learning 0.085s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0177
                 Mean entropy loss: 16.7498
                       Mean reward: -4.42
               Mean episode length: 169.28
Episode_Reward/track_lin_vel_xy_exp: 0.0903
Episode_Reward/track_ang_vel_z_exp: 0.0758
       Episode_Reward/lin_vel_z_l2: -0.0584
      Episode_Reward/ang_vel_xy_l2: -0.2670
     Episode_Reward/dof_torques_l2: -0.0310
         Episode_Reward/dof_acc_l2: -0.1140
     Episode_Reward/action_rate_l2: -0.0882
      Episode_Reward/feet_air_time: -0.0227
Episode_Reward/flat_orientation_l2: -0.0224
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.5319
Metrics/base_velocity/error_vel_yaw: 0.7023
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38400
                    Iteration time: 0.43s
                        Total time: 8.50s
                               ETA: 151.4s

################################################################################
                      [1m Learning iteration 16/300 [0m                       

                       Computation: 5465 steps/s (collection: 0.356s, learning 0.083s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0167
                 Mean entropy loss: 16.7318
                       Mean reward: -4.76
               Mean episode length: 185.18
Episode_Reward/track_lin_vel_xy_exp: 0.1377
Episode_Reward/track_ang_vel_z_exp: 0.0658
       Episode_Reward/lin_vel_z_l2: -0.0650
      Episode_Reward/ang_vel_xy_l2: -0.2944
     Episode_Reward/dof_torques_l2: -0.0392
         Episode_Reward/dof_acc_l2: -0.0942
     Episode_Reward/action_rate_l2: -0.0957
      Episode_Reward/feet_air_time: -0.0223
Episode_Reward/flat_orientation_l2: -0.0303
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.6136
Metrics/base_velocity/error_vel_yaw: 0.8955
      Episode_Termination/time_out: 0.7500
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40800
                    Iteration time: 0.44s
                        Total time: 8.94s
                               ETA: 149.3s

################################################################################
                      [1m Learning iteration 17/300 [0m                       

                       Computation: 5519 steps/s (collection: 0.350s, learning 0.084s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 16.7102
                       Mean reward: -4.94
               Mean episode length: 190.32
Episode_Reward/track_lin_vel_xy_exp: 0.0077
Episode_Reward/track_ang_vel_z_exp: 0.0587
       Episode_Reward/lin_vel_z_l2: -0.0585
      Episode_Reward/ang_vel_xy_l2: -0.2333
     Episode_Reward/dof_torques_l2: -0.0339
         Episode_Reward/dof_acc_l2: -0.0764
     Episode_Reward/action_rate_l2: -0.0765
      Episode_Reward/feet_air_time: -0.0169
Episode_Reward/flat_orientation_l2: -0.0316
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.7029
Metrics/base_velocity/error_vel_yaw: 0.6705
      Episode_Termination/time_out: 0.7500
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43200
                    Iteration time: 0.43s
                        Total time: 9.37s
                               ETA: 147.4s

################################################################################
                      [1m Learning iteration 18/300 [0m                       

                       Computation: 5481 steps/s (collection: 0.354s, learning 0.084s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0179
                 Mean entropy loss: 16.6872
                       Mean reward: -5.18
               Mean episode length: 205.12
Episode_Reward/track_lin_vel_xy_exp: 0.0809
Episode_Reward/track_ang_vel_z_exp: 0.0347
       Episode_Reward/lin_vel_z_l2: -0.0428
      Episode_Reward/ang_vel_xy_l2: -0.1566
     Episode_Reward/dof_torques_l2: -0.0199
         Episode_Reward/dof_acc_l2: -0.0497
     Episode_Reward/action_rate_l2: -0.0479
      Episode_Reward/feet_air_time: -0.0113
Episode_Reward/flat_orientation_l2: -0.0168
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2632
Metrics/base_velocity/error_vel_yaw: 0.4336
      Episode_Termination/time_out: 0.3750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45600
                    Iteration time: 0.44s
                        Total time: 9.81s
                               ETA: 145.6s

################################################################################
                      [1m Learning iteration 19/300 [0m                       

                       Computation: 5632 steps/s (collection: 0.342s, learning 0.084s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0187
               Mean surrogate loss: -0.0174
                 Mean entropy loss: 16.6689
                       Mean reward: -5.29
               Mean episode length: 216.96
Episode_Reward/track_lin_vel_xy_exp: 0.2594
Episode_Reward/track_ang_vel_z_exp: 0.0846
       Episode_Reward/lin_vel_z_l2: -0.0738
      Episode_Reward/ang_vel_xy_l2: -0.3528
     Episode_Reward/dof_torques_l2: -0.0480
         Episode_Reward/dof_acc_l2: -0.1144
     Episode_Reward/action_rate_l2: -0.1135
      Episode_Reward/feet_air_time: -0.0260
Episode_Reward/flat_orientation_l2: -0.0315
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.4854
Metrics/base_velocity/error_vel_yaw: 0.9938
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48000
                    Iteration time: 0.43s
                        Total time: 10.24s
                               ETA: 143.8s

################################################################################
                      [1m Learning iteration 20/300 [0m                       

                       Computation: 5546 steps/s (collection: 0.349s, learning 0.084s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0191
                 Mean entropy loss: 16.6582
                       Mean reward: -5.30
               Mean episode length: 222.24
Episode_Reward/track_lin_vel_xy_exp: 0.2691
Episode_Reward/track_ang_vel_z_exp: 0.0695
       Episode_Reward/lin_vel_z_l2: -0.0618
      Episode_Reward/ang_vel_xy_l2: -0.2847
     Episode_Reward/dof_torques_l2: -0.0384
         Episode_Reward/dof_acc_l2: -0.0890
     Episode_Reward/action_rate_l2: -0.0930
      Episode_Reward/feet_air_time: -0.0205
Episode_Reward/flat_orientation_l2: -0.0216
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.3377
Metrics/base_velocity/error_vel_yaw: 0.8028
      Episode_Termination/time_out: 0.7500
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 50400
                    Iteration time: 0.43s
                        Total time: 10.67s
                               ETA: 142.3s

################################################################################
                      [1m Learning iteration 21/300 [0m                       

                       Computation: 5634 steps/s (collection: 0.340s, learning 0.086s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0169
                 Mean entropy loss: 16.6457
                       Mean reward: -5.30
               Mean episode length: 222.24
Episode_Reward/track_lin_vel_xy_exp: 0.1967
Episode_Reward/track_ang_vel_z_exp: 0.0985
       Episode_Reward/lin_vel_z_l2: -0.0773
      Episode_Reward/ang_vel_xy_l2: -0.3687
     Episode_Reward/dof_torques_l2: -0.0503
         Episode_Reward/dof_acc_l2: -0.1300
     Episode_Reward/action_rate_l2: -0.1227
      Episode_Reward/feet_air_time: -0.0275
Episode_Reward/flat_orientation_l2: -0.0322
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.6002
Metrics/base_velocity/error_vel_yaw: 1.0201
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52800
                    Iteration time: 0.43s
                        Total time: 11.10s
                               ETA: 140.7s

################################################################################
                      [1m Learning iteration 22/300 [0m                       

                       Computation: 5573 steps/s (collection: 0.347s, learning 0.084s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0176
                 Mean entropy loss: 16.6288
                       Mean reward: -5.41
               Mean episode length: 226.66
Episode_Reward/track_lin_vel_xy_exp: 0.0726
Episode_Reward/track_ang_vel_z_exp: 0.1061
       Episode_Reward/lin_vel_z_l2: -0.0752
      Episode_Reward/ang_vel_xy_l2: -0.3664
     Episode_Reward/dof_torques_l2: -0.0523
         Episode_Reward/dof_acc_l2: -0.1208
     Episode_Reward/action_rate_l2: -0.1254
      Episode_Reward/feet_air_time: -0.0297
Episode_Reward/flat_orientation_l2: -0.0251
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.9650
Metrics/base_velocity/error_vel_yaw: 1.0488
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55200
                    Iteration time: 0.43s
                        Total time: 11.53s
                               ETA: 139.3s

################################################################################
                      [1m Learning iteration 23/300 [0m                       

                       Computation: 5501 steps/s (collection: 0.353s, learning 0.083s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0172
                 Mean entropy loss: 16.6056
                       Mean reward: -5.64
               Mean episode length: 243.13
Episode_Reward/track_lin_vel_xy_exp: 0.3444
Episode_Reward/track_ang_vel_z_exp: 0.0912
       Episode_Reward/lin_vel_z_l2: -0.0783
      Episode_Reward/ang_vel_xy_l2: -0.3971
     Episode_Reward/dof_torques_l2: -0.0512
         Episode_Reward/dof_acc_l2: -0.1526
     Episode_Reward/action_rate_l2: -0.1326
      Episode_Reward/feet_air_time: -0.0193
Episode_Reward/flat_orientation_l2: -0.0355
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.6339
Metrics/base_velocity/error_vel_yaw: 1.2120
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57600
                    Iteration time: 0.44s
                        Total time: 11.96s
                               ETA: 138.1s

################################################################################
                      [1m Learning iteration 24/300 [0m                       

                       Computation: 5513 steps/s (collection: 0.351s, learning 0.084s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0188
               Mean surrogate loss: -0.0173
                 Mean entropy loss: 16.5879
                       Mean reward: -5.94
               Mean episode length: 256.38
Episode_Reward/track_lin_vel_xy_exp: 0.1337
Episode_Reward/track_ang_vel_z_exp: 0.1147
       Episode_Reward/lin_vel_z_l2: -0.0725
      Episode_Reward/ang_vel_xy_l2: -0.4303
     Episode_Reward/dof_torques_l2: -0.0571
         Episode_Reward/dof_acc_l2: -0.1475
     Episode_Reward/action_rate_l2: -0.1402
      Episode_Reward/feet_air_time: -0.0340
Episode_Reward/flat_orientation_l2: -0.0476
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.9513
Metrics/base_velocity/error_vel_yaw: 1.1384
      Episode_Termination/time_out: 0.6667
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60000
                    Iteration time: 0.44s
                        Total time: 12.40s
                               ETA: 136.9s

################################################################################
                      [1m Learning iteration 25/300 [0m                       

                       Computation: 5575 steps/s (collection: 0.346s, learning 0.085s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0180
                 Mean entropy loss: 16.5711
                       Mean reward: -5.97
               Mean episode length: 260.80
Episode_Reward/track_lin_vel_xy_exp: 0.4141
Episode_Reward/track_ang_vel_z_exp: 0.1009
       Episode_Reward/lin_vel_z_l2: -0.0628
      Episode_Reward/ang_vel_xy_l2: -0.4638
     Episode_Reward/dof_torques_l2: -0.0530
         Episode_Reward/dof_acc_l2: -0.1604
     Episode_Reward/action_rate_l2: -0.1414
      Episode_Reward/feet_air_time: -0.0351
Episode_Reward/flat_orientation_l2: -0.0410
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.5632
Metrics/base_velocity/error_vel_yaw: 1.3777
      Episode_Termination/time_out: 0.8333
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62400
                    Iteration time: 0.43s
                        Total time: 12.83s
                               ETA: 135.7s

################################################################################
                      [1m Learning iteration 26/300 [0m                       

                       Computation: 5694 steps/s (collection: 0.337s, learning 0.084s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0181
                 Mean entropy loss: 16.5624
                       Mean reward: -6.11
               Mean episode length: 270.12
Episode_Reward/track_lin_vel_xy_exp: 0.4641
Episode_Reward/track_ang_vel_z_exp: 0.1157
       Episode_Reward/lin_vel_z_l2: -0.0731
      Episode_Reward/ang_vel_xy_l2: -0.4685
     Episode_Reward/dof_torques_l2: -0.0534
         Episode_Reward/dof_acc_l2: -0.1710
     Episode_Reward/action_rate_l2: -0.1469
      Episode_Reward/feet_air_time: -0.0377
Episode_Reward/flat_orientation_l2: -0.0362
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.5365
Metrics/base_velocity/error_vel_yaw: 1.3207
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64800
                    Iteration time: 0.42s
                        Total time: 13.25s
                               ETA: 134.5s

################################################################################
                      [1m Learning iteration 27/300 [0m                       

                       Computation: 5639 steps/s (collection: 0.347s, learning 0.079s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0177
                 Mean entropy loss: 16.5520
                       Mean reward: -6.49
               Mean episode length: 288.45
Episode_Reward/track_lin_vel_xy_exp: 0.1106
Episode_Reward/track_ang_vel_z_exp: 0.1299
       Episode_Reward/lin_vel_z_l2: -0.0909
      Episode_Reward/ang_vel_xy_l2: -0.4618
     Episode_Reward/dof_torques_l2: -0.0633
         Episode_Reward/dof_acc_l2: -0.1574
     Episode_Reward/action_rate_l2: -0.1599
      Episode_Reward/feet_air_time: -0.0390
Episode_Reward/flat_orientation_l2: -0.0348
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 1.1394
Metrics/base_velocity/error_vel_yaw: 1.3163
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67200
                    Iteration time: 0.43s
                        Total time: 13.68s
                               ETA: 133.3s

################################################################################
                      [1m Learning iteration 28/300 [0m                       

                       Computation: 5765 steps/s (collection: 0.332s, learning 0.085s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 16.5393
                       Mean reward: -6.51
               Mean episode length: 288.51
Episode_Reward/track_lin_vel_xy_exp: 0.0667
Episode_Reward/track_ang_vel_z_exp: 0.0842
       Episode_Reward/lin_vel_z_l2: -0.0611
      Episode_Reward/ang_vel_xy_l2: -0.3124
     Episode_Reward/dof_torques_l2: -0.0411
         Episode_Reward/dof_acc_l2: -0.1168
     Episode_Reward/action_rate_l2: -0.1005
      Episode_Reward/feet_air_time: -0.0251
Episode_Reward/flat_orientation_l2: -0.0239
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.7060
Metrics/base_velocity/error_vel_yaw: 0.8442
      Episode_Termination/time_out: 0.3333
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 69600
                    Iteration time: 0.42s
                        Total time: 14.09s
                               ETA: 132.2s

################################################################################
                      [1m Learning iteration 29/300 [0m                       

                       Computation: 5606 steps/s (collection: 0.347s, learning 0.081s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0188
                 Mean entropy loss: 16.5236
                       Mean reward: -6.84
               Mean episode length: 306.52
Episode_Reward/track_lin_vel_xy_exp: 0.2172
Episode_Reward/track_ang_vel_z_exp: 0.1390
       Episode_Reward/lin_vel_z_l2: -0.0950
      Episode_Reward/ang_vel_xy_l2: -0.4627
     Episode_Reward/dof_torques_l2: -0.0624
         Episode_Reward/dof_acc_l2: -0.1763
     Episode_Reward/action_rate_l2: -0.1587
      Episode_Reward/feet_air_time: -0.0366
Episode_Reward/flat_orientation_l2: -0.0356
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.9533
Metrics/base_velocity/error_vel_yaw: 1.2630
      Episode_Termination/time_out: 0.6250
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72000
                    Iteration time: 0.43s
                        Total time: 14.52s
                               ETA: 131.2s

################################################################################
                      [1m Learning iteration 30/300 [0m                       

                       Computation: 5579 steps/s (collection: 0.348s, learning 0.082s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0175
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 16.5155
                       Mean reward: -6.95
               Mean episode length: 310.18
Episode_Reward/track_lin_vel_xy_exp: 0.1396
Episode_Reward/track_ang_vel_z_exp: 0.0836
       Episode_Reward/lin_vel_z_l2: -0.0698
      Episode_Reward/ang_vel_xy_l2: -0.3557
     Episode_Reward/dof_torques_l2: -0.0468
         Episode_Reward/dof_acc_l2: -0.1268
     Episode_Reward/action_rate_l2: -0.1171
      Episode_Reward/feet_air_time: -0.0274
Episode_Reward/flat_orientation_l2: -0.0438
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.8933
Metrics/base_velocity/error_vel_yaw: 1.0581
      Episode_Termination/time_out: 0.6250
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74400
                    Iteration time: 0.43s
                        Total time: 14.95s
                               ETA: 130.2s

################################################################################
                      [1m Learning iteration 31/300 [0m                       

                       Computation: 5752 steps/s (collection: 0.334s, learning 0.083s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0172
                 Mean entropy loss: 16.5028
                       Mean reward: -6.94
               Mean episode length: 314.81
Episode_Reward/track_lin_vel_xy_exp: 0.6323
Episode_Reward/track_ang_vel_z_exp: 0.1574
       Episode_Reward/lin_vel_z_l2: -0.0909
      Episode_Reward/ang_vel_xy_l2: -0.5400
     Episode_Reward/dof_torques_l2: -0.0679
         Episode_Reward/dof_acc_l2: -0.2125
     Episode_Reward/action_rate_l2: -0.1823
      Episode_Reward/feet_air_time: -0.0464
Episode_Reward/flat_orientation_l2: -0.0447
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.6872
Metrics/base_velocity/error_vel_yaw: 1.4384
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76800
                    Iteration time: 0.42s
                        Total time: 15.37s
                               ETA: 129.2s

################################################################################
                      [1m Learning iteration 32/300 [0m                       

                       Computation: 5745 steps/s (collection: 0.337s, learning 0.081s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0167
                 Mean entropy loss: 16.4876
                       Mean reward: -7.12
               Mean episode length: 323.82
Episode_Reward/track_lin_vel_xy_exp: 0.2625
Episode_Reward/track_ang_vel_z_exp: 0.1831
       Episode_Reward/lin_vel_z_l2: -0.0896
      Episode_Reward/ang_vel_xy_l2: -0.5321
     Episode_Reward/dof_torques_l2: -0.0772
         Episode_Reward/dof_acc_l2: -0.1653
     Episode_Reward/action_rate_l2: -0.1822
      Episode_Reward/feet_air_time: -0.0443
Episode_Reward/flat_orientation_l2: -0.0458
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 1.0707
Metrics/base_velocity/error_vel_yaw: 1.2318
      Episode_Termination/time_out: 0.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79200
                    Iteration time: 0.42s
                        Total time: 15.79s
                               ETA: 128.2s

################################################################################
                      [1m Learning iteration 33/300 [0m                       

                       Computation: 5802 steps/s (collection: 0.331s, learning 0.083s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0109
               Mean surrogate loss: -0.0195
                 Mean entropy loss: 16.4697
                       Mean reward: -7.12
               Mean episode length: 323.82
Episode_Reward/track_lin_vel_xy_exp: 0.2637
Episode_Reward/track_ang_vel_z_exp: 0.1997
       Episode_Reward/lin_vel_z_l2: -0.0886
      Episode_Reward/ang_vel_xy_l2: -0.5433
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.1633
     Episode_Reward/action_rate_l2: -0.1900
      Episode_Reward/feet_air_time: -0.0452
Episode_Reward/flat_orientation_l2: -0.0424
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 1.0444
Metrics/base_velocity/error_vel_yaw: 1.2207
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81600
                    Iteration time: 0.41s
                        Total time: 16.20s
                               ETA: 127.2s

################################################################################
                      [1m Learning iteration 34/300 [0m                       

                       Computation: 5646 steps/s (collection: 0.343s, learning 0.082s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0166
                 Mean entropy loss: 16.4495
                       Mean reward: -7.38
               Mean episode length: 347.04
Episode_Reward/track_lin_vel_xy_exp: 0.4406
Episode_Reward/track_ang_vel_z_exp: 0.1833
       Episode_Reward/lin_vel_z_l2: -0.0907
      Episode_Reward/ang_vel_xy_l2: -0.5884
     Episode_Reward/dof_torques_l2: -0.0821
         Episode_Reward/dof_acc_l2: -0.2091
     Episode_Reward/action_rate_l2: -0.1988
      Episode_Reward/feet_air_time: -0.0351
Episode_Reward/flat_orientation_l2: -0.0553
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.9850
Metrics/base_velocity/error_vel_yaw: 1.4636
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84000
                    Iteration time: 0.43s
                        Total time: 16.62s
                               ETA: 126.3s

################################################################################
                      [1m Learning iteration 35/300 [0m                       

                       Computation: 5761 steps/s (collection: 0.334s, learning 0.082s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0201
                 Mean entropy loss: 16.4217
                       Mean reward: -7.48
               Mean episode length: 355.54
Episode_Reward/track_lin_vel_xy_exp: 0.2116
Episode_Reward/track_ang_vel_z_exp: 0.1367
       Episode_Reward/lin_vel_z_l2: -0.0870
      Episode_Reward/ang_vel_xy_l2: -0.5819
     Episode_Reward/dof_torques_l2: -0.0825
         Episode_Reward/dof_acc_l2: -0.2340
     Episode_Reward/action_rate_l2: -0.2012
      Episode_Reward/feet_air_time: -0.0505
Episode_Reward/flat_orientation_l2: -0.0465
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 1.2169
Metrics/base_velocity/error_vel_yaw: 1.8364
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86400
                    Iteration time: 0.42s
                        Total time: 17.04s
                               ETA: 125.4s

################################################################################
                      [1m Learning iteration 36/300 [0m                       

                       Computation: 5656 steps/s (collection: 0.342s, learning 0.083s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 16.4036
                       Mean reward: -8.03
               Mean episode length: 378.53
Episode_Reward/track_lin_vel_xy_exp: 0.0707
Episode_Reward/track_ang_vel_z_exp: 0.1263
       Episode_Reward/lin_vel_z_l2: -0.0845
      Episode_Reward/ang_vel_xy_l2: -0.5048
     Episode_Reward/dof_torques_l2: -0.0645
         Episode_Reward/dof_acc_l2: -0.1929
     Episode_Reward/action_rate_l2: -0.1778
      Episode_Reward/feet_air_time: -0.0452
Episode_Reward/flat_orientation_l2: -0.0351
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 1.4913
Metrics/base_velocity/error_vel_yaw: 1.6523
      Episode_Termination/time_out: 0.4583
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 88800
                    Iteration time: 0.42s
                        Total time: 17.46s
                               ETA: 124.6s

################################################################################
                      [1m Learning iteration 37/300 [0m                       

                       Computation: 5549 steps/s (collection: 0.352s, learning 0.081s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0175
                 Mean entropy loss: 16.3872
                       Mean reward: -9.05
               Mean episode length: 423.44
Episode_Reward/track_lin_vel_xy_exp: 0.0743
Episode_Reward/track_ang_vel_z_exp: 0.1227
       Episode_Reward/lin_vel_z_l2: -0.0828
      Episode_Reward/ang_vel_xy_l2: -0.5192
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.1998
     Episode_Reward/action_rate_l2: -0.1767
      Episode_Reward/feet_air_time: -0.0449
Episode_Reward/flat_orientation_l2: -0.0390
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 1.4795
Metrics/base_velocity/error_vel_yaw: 1.6185
      Episode_Termination/time_out: 0.5000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91200
                    Iteration time: 0.43s
                        Total time: 17.90s
                               ETA: 123.9s

################################################################################
                      [1m Learning iteration 38/300 [0m                       

                       Computation: 5532 steps/s (collection: 0.350s, learning 0.084s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0417
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 16.3751
                       Mean reward: -9.56
               Mean episode length: 446.44
Episode_Reward/track_lin_vel_xy_exp: 0.0778
Episode_Reward/track_ang_vel_z_exp: 0.1191
       Episode_Reward/lin_vel_z_l2: -0.0711
      Episode_Reward/ang_vel_xy_l2: -0.3624
     Episode_Reward/dof_torques_l2: -0.0461
         Episode_Reward/dof_acc_l2: -0.1409
     Episode_Reward/action_rate_l2: -0.1291
      Episode_Reward/feet_air_time: -0.0301
Episode_Reward/flat_orientation_l2: -0.0469
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.9203
Metrics/base_velocity/error_vel_yaw: 0.9534
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93600
                    Iteration time: 0.43s
                        Total time: 18.33s
                               ETA: 123.1s

################################################################################
                      [1m Learning iteration 39/300 [0m                       

                       Computation: 5719 steps/s (collection: 0.338s, learning 0.081s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0214
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 16.3632
                       Mean reward: -9.88
               Mean episode length: 460.45
Episode_Reward/track_lin_vel_xy_exp: 0.0978
Episode_Reward/track_ang_vel_z_exp: 0.1059
       Episode_Reward/lin_vel_z_l2: -0.0703
      Episode_Reward/ang_vel_xy_l2: -0.3623
     Episode_Reward/dof_torques_l2: -0.0487
         Episode_Reward/dof_acc_l2: -0.1370
     Episode_Reward/action_rate_l2: -0.1291
      Episode_Reward/feet_air_time: -0.0305
Episode_Reward/flat_orientation_l2: -0.0289
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.9163
Metrics/base_velocity/error_vel_yaw: 1.0374
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96000
                    Iteration time: 0.42s
                        Total time: 18.75s
                               ETA: 122.3s

################################################################################
                      [1m Learning iteration 40/300 [0m                       

                       Computation: 5444 steps/s (collection: 0.358s, learning 0.082s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0415
               Mean surrogate loss: -0.0170
                 Mean entropy loss: 16.3525
                       Mean reward: -10.57
               Mean episode length: 497.48
Episode_Reward/track_lin_vel_xy_exp: 0.0879
Episode_Reward/track_ang_vel_z_exp: 0.1131
       Episode_Reward/lin_vel_z_l2: -0.0722
      Episode_Reward/ang_vel_xy_l2: -0.4040
     Episode_Reward/dof_torques_l2: -0.0557
         Episode_Reward/dof_acc_l2: -0.1467
     Episode_Reward/action_rate_l2: -0.1421
      Episode_Reward/feet_air_time: -0.0350
Episode_Reward/flat_orientation_l2: -0.0380
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 1.1959
Metrics/base_velocity/error_vel_yaw: 1.1701
      Episode_Termination/time_out: 0.2500
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 98400
                    Iteration time: 0.44s
                        Total time: 19.19s
                               ETA: 121.7s

################################################################################
                      [1m Learning iteration 41/300 [0m                       

                       Computation: 5637 steps/s (collection: 0.347s, learning 0.079s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 16.3424
                       Mean reward: -10.76
               Mean episode length: 509.17
Episode_Reward/track_lin_vel_xy_exp: 0.1365
Episode_Reward/track_ang_vel_z_exp: 0.0793
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.2790
     Episode_Reward/dof_torques_l2: -0.0451
         Episode_Reward/dof_acc_l2: -0.1002
     Episode_Reward/action_rate_l2: -0.1042
      Episode_Reward/feet_air_time: -0.0249
Episode_Reward/flat_orientation_l2: -0.0300
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.6776
Metrics/base_velocity/error_vel_yaw: 0.8801
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100800
                    Iteration time: 0.43s
                        Total time: 19.62s
                               ETA: 121.0s

################################################################################
                      [1m Learning iteration 42/300 [0m                       

                       Computation: 5423 steps/s (collection: 0.362s, learning 0.081s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0444
               Mean surrogate loss: -0.0188
                 Mean entropy loss: 16.3315
                       Mean reward: -11.76
               Mean episode length: 569.39
Episode_Reward/track_lin_vel_xy_exp: 0.1687
Episode_Reward/track_ang_vel_z_exp: 0.1054
       Episode_Reward/lin_vel_z_l2: -0.0634
      Episode_Reward/ang_vel_xy_l2: -0.3688
     Episode_Reward/dof_torques_l2: -0.0530
         Episode_Reward/dof_acc_l2: -0.1315
     Episode_Reward/action_rate_l2: -0.1317
      Episode_Reward/feet_air_time: -0.0318
Episode_Reward/flat_orientation_l2: -0.0309
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.8370
Metrics/base_velocity/error_vel_yaw: 1.0857
      Episode_Termination/time_out: 0.2083
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103200
                    Iteration time: 0.44s
                        Total time: 20.06s
                               ETA: 120.4s

################################################################################
                      [1m Learning iteration 43/300 [0m                       

                       Computation: 5455 steps/s (collection: 0.358s, learning 0.082s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0420
               Mean surrogate loss: -0.0173
                 Mean entropy loss: 16.3270
                       Mean reward: -12.32
               Mean episode length: 596.76
Episode_Reward/track_lin_vel_xy_exp: 0.1721
Episode_Reward/track_ang_vel_z_exp: 0.1129
       Episode_Reward/lin_vel_z_l2: -0.0720
      Episode_Reward/ang_vel_xy_l2: -0.4120
     Episode_Reward/dof_torques_l2: -0.0561
         Episode_Reward/dof_acc_l2: -0.1580
     Episode_Reward/action_rate_l2: -0.1465
      Episode_Reward/feet_air_time: -0.0365
Episode_Reward/flat_orientation_l2: -0.0396
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 1.0295
Metrics/base_velocity/error_vel_yaw: 1.2461
      Episode_Termination/time_out: 0.1250
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105600
                    Iteration time: 0.44s
                        Total time: 20.50s
                               ETA: 119.7s

################################################################################
                      [1m Learning iteration 44/300 [0m                       

                       Computation: 2659 steps/s (collection: 0.819s, learning 0.084s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0690
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 16.3224
                       Mean reward: -12.22
               Mean episode length: 589.62
Episode_Reward/track_lin_vel_xy_exp: 0.1021
Episode_Reward/track_ang_vel_z_exp: 0.0775
       Episode_Reward/lin_vel_z_l2: -0.0509
      Episode_Reward/ang_vel_xy_l2: -0.2552
     Episode_Reward/dof_torques_l2: -0.0350
         Episode_Reward/dof_acc_l2: -0.0959
     Episode_Reward/action_rate_l2: -0.0906
      Episode_Reward/feet_air_time: -0.0222
Episode_Reward/flat_orientation_l2: -0.0259
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.5910
Metrics/base_velocity/error_vel_yaw: 0.7282
      Episode_Termination/time_out: 0.1667
  Episode_Termination/base_contact: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108000
                    Iteration time: 0.90s
                        Total time: 21.40s
                               ETA: 121.8s

################################################################################
                      [1m Learning iteration 45/300 [0m                       

                       Computation: 5232 steps/s (collection: 0.377s, learning 0.082s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0891
               Mean surrogate loss: -0.0187
                 Mean entropy loss: 16.3219
                       Mean reward: -10.10
               Mean episode length: 481.51
Episode_Reward/track_lin_vel_xy_exp: 0.0887
Episode_Reward/track_ang_vel_z_exp: 0.0673
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.2079
     Episode_Reward/dof_torques_l2: -0.0298
         Episode_Reward/dof_acc_l2: -0.0870
     Episode_Reward/action_rate_l2: -0.0760
      Episode_Reward/feet_air_time: -0.0191
Episode_Reward/flat_orientation_l2: -0.0184
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.4937
Metrics/base_velocity/error_vel_yaw: 0.5887
      Episode_Termination/time_out: 0.1667
  Episode_Termination/base_contact: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110400
                    Iteration time: 0.46s
                        Total time: 21.86s
                               ETA: 121.2s

################################################################################
                      [1m Learning iteration 46/300 [0m                       

                       Computation: 5016 steps/s (collection: 0.395s, learning 0.083s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0979
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 16.3226
                       Mean reward: -6.48
               Mean episode length: 313.00
Episode_Reward/track_lin_vel_xy_exp: 0.0945
Episode_Reward/track_ang_vel_z_exp: 0.0624
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.2128
     Episode_Reward/dof_torques_l2: -0.0291
         Episode_Reward/dof_acc_l2: -0.0845
     Episode_Reward/action_rate_l2: -0.0759
      Episode_Reward/feet_air_time: -0.0187
Episode_Reward/flat_orientation_l2: -0.0203
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.4989
Metrics/base_velocity/error_vel_yaw: 0.6132
      Episode_Termination/time_out: 0.0417
  Episode_Termination/base_contact: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112800
                    Iteration time: 0.48s
                        Total time: 22.34s
                               ETA: 120.7s

################################################################################
                      [1m Learning iteration 47/300 [0m                       

                       Computation: 4922 steps/s (collection: 0.405s, learning 0.082s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.1138
               Mean surrogate loss: -0.0163
                 Mean entropy loss: 16.3213
                       Mean reward: -3.81
               Mean episode length: 167.83
Episode_Reward/track_lin_vel_xy_exp: 0.0370
Episode_Reward/track_ang_vel_z_exp: 0.0302
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.1004
     Episode_Reward/dof_torques_l2: -0.0136
         Episode_Reward/dof_acc_l2: -0.0401
     Episode_Reward/action_rate_l2: -0.0349
      Episode_Reward/feet_air_time: -0.0085
Episode_Reward/flat_orientation_l2: -0.0106
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.2339
Metrics/base_velocity/error_vel_yaw: 0.2745
      Episode_Termination/time_out: 0.1250
  Episode_Termination/base_contact: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 115200
                    Iteration time: 0.49s
                        Total time: 22.83s
                               ETA: 120.3s

################################################################################
                      [1m Learning iteration 48/300 [0m                       

                       Computation: 4940 steps/s (collection: 0.402s, learning 0.084s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0935
               Mean surrogate loss: -0.0172
                 Mean entropy loss: 16.3217
                       Mean reward: -2.17
               Mean episode length: 86.11
Episode_Reward/track_lin_vel_xy_exp: 0.0225
Episode_Reward/track_ang_vel_z_exp: 0.0210
       Episode_Reward/lin_vel_z_l2: -0.0280
      Episode_Reward/ang_vel_xy_l2: -0.0649
     Episode_Reward/dof_torques_l2: -0.0090
         Episode_Reward/dof_acc_l2: -0.0289
     Episode_Reward/action_rate_l2: -0.0231
      Episode_Reward/feet_air_time: -0.0054
Episode_Reward/flat_orientation_l2: -0.0079
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1642
Metrics/base_velocity/error_vel_yaw: 0.1779
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117600
                    Iteration time: 0.49s
                        Total time: 23.31s
                               ETA: 119.9s

################################################################################
                      [1m Learning iteration 49/300 [0m                       

                       Computation: 4944 steps/s (collection: 0.401s, learning 0.084s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0726
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 16.3209
                       Mean reward: -1.13
               Mean episode length: 27.22
Episode_Reward/track_lin_vel_xy_exp: 0.0060
Episode_Reward/track_ang_vel_z_exp: 0.0057
       Episode_Reward/lin_vel_z_l2: -0.0232
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.0029
         Episode_Reward/dof_acc_l2: -0.0118
     Episode_Reward/action_rate_l2: -0.0070
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: -0.0037
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0546
Metrics/base_velocity/error_vel_yaw: 0.0574
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120000
                    Iteration time: 0.49s
                        Total time: 23.80s
                               ETA: 119.5s

################################################################################
                      [1m Learning iteration 50/300 [0m                       

                       Computation: 4868 steps/s (collection: 0.410s, learning 0.083s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0377
               Mean surrogate loss: -0.0172
                 Mean entropy loss: 16.3227
                       Mean reward: -0.98
               Mean episode length: 21.22
Episode_Reward/track_lin_vel_xy_exp: 0.0064
Episode_Reward/track_ang_vel_z_exp: 0.0042
       Episode_Reward/lin_vel_z_l2: -0.0228
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.0021
         Episode_Reward/dof_acc_l2: -0.0087
     Episode_Reward/action_rate_l2: -0.0048
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: -0.0029
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0338
Metrics/base_velocity/error_vel_yaw: 0.0391
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122400
                    Iteration time: 0.49s
                        Total time: 24.29s
                               ETA: 119.1s

################################################################################
                      [1m Learning iteration 51/300 [0m                       

                       Computation: 4867 steps/s (collection: 0.411s, learning 0.082s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 16.3271
                       Mean reward: -0.98
               Mean episode length: 19.30
Episode_Reward/track_lin_vel_xy_exp: 0.0052
Episode_Reward/track_ang_vel_z_exp: 0.0044
       Episode_Reward/lin_vel_z_l2: -0.0228
      Episode_Reward/ang_vel_xy_l2: -0.0154
     Episode_Reward/dof_torques_l2: -0.0019
         Episode_Reward/dof_acc_l2: -0.0101
     Episode_Reward/action_rate_l2: -0.0048
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: -0.0027
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0356
Metrics/base_velocity/error_vel_yaw: 0.0361
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124800
                    Iteration time: 0.49s
                        Total time: 24.78s
                               ETA: 118.7s

################################################################################
                      [1m Learning iteration 52/300 [0m                       

                       Computation: 4853 steps/s (collection: 0.411s, learning 0.084s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0208
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 16.3279
                       Mean reward: -0.95
               Mean episode length: 17.32
Episode_Reward/track_lin_vel_xy_exp: 0.0050
Episode_Reward/track_ang_vel_z_exp: 0.0037
       Episode_Reward/lin_vel_z_l2: -0.0231
      Episode_Reward/ang_vel_xy_l2: -0.0145
     Episode_Reward/dof_torques_l2: -0.0016
         Episode_Reward/dof_acc_l2: -0.0099
     Episode_Reward/action_rate_l2: -0.0041
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: -0.0022
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0277
Metrics/base_velocity/error_vel_yaw: 0.0287
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127200
                    Iteration time: 0.49s
                        Total time: 25.28s
                               ETA: 118.3s

################################################################################
                      [1m Learning iteration 53/300 [0m                       

                       Computation: 4857 steps/s (collection: 0.413s, learning 0.081s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0189
               Mean surrogate loss: -0.0172
                 Mean entropy loss: 16.3247
                       Mean reward: -0.91
               Mean episode length: 16.31
Episode_Reward/track_lin_vel_xy_exp: 0.0051
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0227
      Episode_Reward/ang_vel_xy_l2: -0.0138
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0089
     Episode_Reward/action_rate_l2: -0.0038
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0021
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0268
Metrics/base_velocity/error_vel_yaw: 0.0272
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 129600
                    Iteration time: 0.49s
                        Total time: 25.77s
                               ETA: 117.9s

################################################################################
                      [1m Learning iteration 54/300 [0m                       

                       Computation: 4851 steps/s (collection: 0.411s, learning 0.083s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0175
                 Mean entropy loss: 16.3183
                       Mean reward: -0.96
               Mean episode length: 15.15
Episode_Reward/track_lin_vel_xy_exp: 0.0041
Episode_Reward/track_ang_vel_z_exp: 0.0031
       Episode_Reward/lin_vel_z_l2: -0.0232
      Episode_Reward/ang_vel_xy_l2: -0.0141
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0102
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0019
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0249
Metrics/base_velocity/error_vel_yaw: 0.0266
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132000
                    Iteration time: 0.49s
                        Total time: 26.27s
                               ETA: 117.5s

################################################################################
                      [1m Learning iteration 55/300 [0m                       

                       Computation: 4880 steps/s (collection: 0.410s, learning 0.082s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0166
                 Mean entropy loss: 16.3106
                       Mean reward: -0.92
               Mean episode length: 14.67
Episode_Reward/track_lin_vel_xy_exp: 0.0040
Episode_Reward/track_ang_vel_z_exp: 0.0035
       Episode_Reward/lin_vel_z_l2: -0.0228
      Episode_Reward/ang_vel_xy_l2: -0.0135
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0095
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0020
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0259
Metrics/base_velocity/error_vel_yaw: 0.0246
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 134400
                    Iteration time: 0.49s
                        Total time: 26.76s
                               ETA: 117.1s

################################################################################
                      [1m Learning iteration 56/300 [0m                       

                       Computation: 4885 steps/s (collection: 0.409s, learning 0.082s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0232
                 Mean entropy loss: 16.2901
                       Mean reward: -0.91
               Mean episode length: 16.16
Episode_Reward/track_lin_vel_xy_exp: 0.0045
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0226
      Episode_Reward/ang_vel_xy_l2: -0.0137
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0096
     Episode_Reward/action_rate_l2: -0.0038
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0021
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0263
Metrics/base_velocity/error_vel_yaw: 0.0266
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136800
                    Iteration time: 0.49s
                        Total time: 27.25s
                               ETA: 116.6s

################################################################################
                      [1m Learning iteration 57/300 [0m                       

                       Computation: 4868 steps/s (collection: 0.411s, learning 0.082s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0177
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 16.2828
                       Mean reward: -0.91
               Mean episode length: 15.44
Episode_Reward/track_lin_vel_xy_exp: 0.0053
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0230
      Episode_Reward/ang_vel_xy_l2: -0.0137
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0038
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0021
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0252
Metrics/base_velocity/error_vel_yaw: 0.0268
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139200
                    Iteration time: 0.49s
                        Total time: 27.74s
                               ETA: 116.2s

################################################################################
                      [1m Learning iteration 58/300 [0m                       

                       Computation: 4852 steps/s (collection: 0.411s, learning 0.084s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0166
                 Mean entropy loss: 16.2772
                       Mean reward: -0.92
               Mean episode length: 15.37
Episode_Reward/track_lin_vel_xy_exp: 0.0044
Episode_Reward/track_ang_vel_z_exp: 0.0030
       Episode_Reward/lin_vel_z_l2: -0.0231
      Episode_Reward/ang_vel_xy_l2: -0.0128
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0099
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0019
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0248
Metrics/base_velocity/error_vel_yaw: 0.0277
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141600
                    Iteration time: 0.49s
                        Total time: 28.24s
                               ETA: 115.8s

################################################################################
                      [1m Learning iteration 59/300 [0m                       

                       Computation: 4846 steps/s (collection: 0.411s, learning 0.085s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0183
                 Mean entropy loss: 16.2684
                       Mean reward: -0.91
               Mean episode length: 14.77
Episode_Reward/track_lin_vel_xy_exp: 0.0041
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0230
      Episode_Reward/ang_vel_xy_l2: -0.0131
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0019
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0253
Metrics/base_velocity/error_vel_yaw: 0.0257
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144000
                    Iteration time: 0.50s
                        Total time: 28.73s
                               ETA: 115.4s

################################################################################
                      [1m Learning iteration 60/300 [0m                       

                       Computation: 4889 steps/s (collection: 0.409s, learning 0.082s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0211
                 Mean entropy loss: 16.2527
                       Mean reward: -0.93
               Mean episode length: 14.33
Episode_Reward/track_lin_vel_xy_exp: 0.0039
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0238
      Episode_Reward/ang_vel_xy_l2: -0.0131
     Episode_Reward/dof_torques_l2: -0.0012
         Episode_Reward/dof_acc_l2: -0.0098
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0017
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0242
Metrics/base_velocity/error_vel_yaw: 0.0234
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146400
                    Iteration time: 0.49s
                        Total time: 29.22s
                               ETA: 115.0s

################################################################################
                      [1m Learning iteration 61/300 [0m                       

                       Computation: 4862 steps/s (collection: 0.411s, learning 0.083s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 16.2451
                       Mean reward: -0.90
               Mean episode length: 14.70
Episode_Reward/track_lin_vel_xy_exp: 0.0043
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0231
      Episode_Reward/ang_vel_xy_l2: -0.0129
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0087
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0018
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0236
Metrics/base_velocity/error_vel_yaw: 0.0233
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 148800
                    Iteration time: 0.49s
                        Total time: 29.72s
                               ETA: 114.6s

################################################################################
                      [1m Learning iteration 62/300 [0m                       

                       Computation: 4849 steps/s (collection: 0.410s, learning 0.085s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 16.2306
                       Mean reward: -0.90
               Mean episode length: 15.08
Episode_Reward/track_lin_vel_xy_exp: 0.0046
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0230
      Episode_Reward/ang_vel_xy_l2: -0.0130
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0019
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0242
Metrics/base_velocity/error_vel_yaw: 0.0244
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151200
                    Iteration time: 0.49s
                        Total time: 30.21s
                               ETA: 114.1s

################################################################################
                      [1m Learning iteration 63/300 [0m                       

                       Computation: 4845 steps/s (collection: 0.411s, learning 0.084s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 16.2236
                       Mean reward: -0.91
               Mean episode length: 14.62
Episode_Reward/track_lin_vel_xy_exp: 0.0040
Episode_Reward/track_ang_vel_z_exp: 0.0032
       Episode_Reward/lin_vel_z_l2: -0.0223
      Episode_Reward/ang_vel_xy_l2: -0.0134
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0019
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0249
Metrics/base_velocity/error_vel_yaw: 0.0247
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153600
                    Iteration time: 0.50s
                        Total time: 30.71s
                               ETA: 113.7s

################################################################################
                      [1m Learning iteration 64/300 [0m                       

                       Computation: 4842 steps/s (collection: 0.411s, learning 0.085s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 16.2129
                       Mean reward: -0.91
               Mean episode length: 14.86
Episode_Reward/track_lin_vel_xy_exp: 0.0051
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0222
      Episode_Reward/ang_vel_xy_l2: -0.0131
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0100
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0020
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0231
Metrics/base_velocity/error_vel_yaw: 0.0243
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156000
                    Iteration time: 0.50s
                        Total time: 31.20s
                               ETA: 113.3s

################################################################################
                      [1m Learning iteration 65/300 [0m                       

                       Computation: 4866 steps/s (collection: 0.411s, learning 0.083s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 16.1906
                       Mean reward: -0.92
               Mean episode length: 15.00
Episode_Reward/track_lin_vel_xy_exp: 0.0038
Episode_Reward/track_ang_vel_z_exp: 0.0032
       Episode_Reward/lin_vel_z_l2: -0.0219
      Episode_Reward/ang_vel_xy_l2: -0.0130
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0100
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0020
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0260
Metrics/base_velocity/error_vel_yaw: 0.0246
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158400
                    Iteration time: 0.49s
                        Total time: 31.70s
                               ETA: 112.9s

################################################################################
                      [1m Learning iteration 66/300 [0m                       

                       Computation: 4862 steps/s (collection: 0.409s, learning 0.084s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 16.1819
                       Mean reward: -0.86
               Mean episode length: 14.69
Episode_Reward/track_lin_vel_xy_exp: 0.0047
Episode_Reward/track_ang_vel_z_exp: 0.0035
       Episode_Reward/lin_vel_z_l2: -0.0220
      Episode_Reward/ang_vel_xy_l2: -0.0124
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0020
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0243
Metrics/base_velocity/error_vel_yaw: 0.0237
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160800
                    Iteration time: 0.49s
                        Total time: 32.19s
                               ETA: 112.4s

################################################################################
                      [1m Learning iteration 67/300 [0m                       

                       Computation: 4831 steps/s (collection: 0.414s, learning 0.083s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 16.1732
                       Mean reward: -0.89
               Mean episode length: 14.84
Episode_Reward/track_lin_vel_xy_exp: 0.0040
Episode_Reward/track_ang_vel_z_exp: 0.0032
       Episode_Reward/lin_vel_z_l2: -0.0221
      Episode_Reward/ang_vel_xy_l2: -0.0126
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0095
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0020
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0255
Metrics/base_velocity/error_vel_yaw: 0.0250
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 163200
                    Iteration time: 0.50s
                        Total time: 32.69s
                               ETA: 112.0s

################################################################################
                      [1m Learning iteration 68/300 [0m                       

                       Computation: 4830 steps/s (collection: 0.412s, learning 0.085s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0166
                 Mean entropy loss: 16.1629
                       Mean reward: -0.86
               Mean episode length: 15.30
Episode_Reward/track_lin_vel_xy_exp: 0.0045
Episode_Reward/track_ang_vel_z_exp: 0.0035
       Episode_Reward/lin_vel_z_l2: -0.0212
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0099
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0021
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0251
Metrics/base_velocity/error_vel_yaw: 0.0243
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165600
                    Iteration time: 0.50s
                        Total time: 33.18s
                               ETA: 111.6s

################################################################################
                      [1m Learning iteration 69/300 [0m                       

                       Computation: 4901 steps/s (collection: 0.411s, learning 0.078s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0242
                 Mean entropy loss: 16.1403
                       Mean reward: -0.87
               Mean episode length: 14.79
Episode_Reward/track_lin_vel_xy_exp: 0.0044
Episode_Reward/track_ang_vel_z_exp: 0.0031
       Episode_Reward/lin_vel_z_l2: -0.0217
      Episode_Reward/ang_vel_xy_l2: -0.0126
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0020
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0244
Metrics/base_velocity/error_vel_yaw: 0.0251
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 168000
                    Iteration time: 0.49s
                        Total time: 33.67s
                               ETA: 111.1s

################################################################################
                      [1m Learning iteration 70/300 [0m                       

                       Computation: 4888 steps/s (collection: 0.408s, learning 0.083s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0173
                 Mean entropy loss: 16.1312
                       Mean reward: -0.89
               Mean episode length: 14.75
Episode_Reward/track_lin_vel_xy_exp: 0.0038
Episode_Reward/track_ang_vel_z_exp: 0.0031
       Episode_Reward/lin_vel_z_l2: -0.0216
      Episode_Reward/ang_vel_xy_l2: -0.0129
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0099
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0021
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0256
Metrics/base_velocity/error_vel_yaw: 0.0254
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170400
                    Iteration time: 0.49s
                        Total time: 34.16s
                               ETA: 110.7s

################################################################################
                      [1m Learning iteration 71/300 [0m                       

                       Computation: 4857 steps/s (collection: 0.411s, learning 0.083s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0185
                 Mean entropy loss: 16.1214
                       Mean reward: -0.87
               Mean episode length: 14.63
Episode_Reward/track_lin_vel_xy_exp: 0.0041
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0215
      Episode_Reward/ang_vel_xy_l2: -0.0130
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0092
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0021
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0250
Metrics/base_velocity/error_vel_yaw: 0.0245
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172800
                    Iteration time: 0.49s
                        Total time: 34.66s
                               ETA: 110.2s

################################################################################
                      [1m Learning iteration 72/300 [0m                       

                       Computation: 4856 steps/s (collection: 0.410s, learning 0.084s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0116
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 16.1156
                       Mean reward: -0.86
               Mean episode length: 14.78
Episode_Reward/track_lin_vel_xy_exp: 0.0043
Episode_Reward/track_ang_vel_z_exp: 0.0032
       Episode_Reward/lin_vel_z_l2: -0.0206
      Episode_Reward/ang_vel_xy_l2: -0.0125
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0092
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0021
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0263
Metrics/base_velocity/error_vel_yaw: 0.0263
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175200
                    Iteration time: 0.49s
                        Total time: 35.15s
                               ETA: 109.8s

################################################################################
                      [1m Learning iteration 73/300 [0m                       

                       Computation: 4870 steps/s (collection: 0.411s, learning 0.082s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 16.1122
                       Mean reward: -0.83
               Mean episode length: 15.25
Episode_Reward/track_lin_vel_xy_exp: 0.0043
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0201
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0089
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0023
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0258
Metrics/base_velocity/error_vel_yaw: 0.0244
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177600
                    Iteration time: 0.49s
                        Total time: 35.65s
                               ETA: 109.3s

################################################################################
                      [1m Learning iteration 74/300 [0m                       

                       Computation: 4868 steps/s (collection: 0.410s, learning 0.083s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 16.1088
                       Mean reward: -0.85
               Mean episode length: 15.67
Episode_Reward/track_lin_vel_xy_exp: 0.0048
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0201
      Episode_Reward/ang_vel_xy_l2: -0.0131
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0097
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0028
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0267
Metrics/base_velocity/error_vel_yaw: 0.0270
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180000
                    Iteration time: 0.49s
                        Total time: 36.14s
                               ETA: 108.9s

################################################################################
                      [1m Learning iteration 75/300 [0m                       

                       Computation: 4856 steps/s (collection: 0.410s, learning 0.084s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 16.1029
                       Mean reward: -0.86
               Mean episode length: 15.37
Episode_Reward/track_lin_vel_xy_exp: 0.0037
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0194
      Episode_Reward/ang_vel_xy_l2: -0.0130
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0098
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0024
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0277
Metrics/base_velocity/error_vel_yaw: 0.0265
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 182400
                    Iteration time: 0.49s
                        Total time: 36.63s
                               ETA: 108.5s

################################################################################
                      [1m Learning iteration 76/300 [0m                       

                       Computation: 4860 steps/s (collection: 0.410s, learning 0.083s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0203
                 Mean entropy loss: 16.0869
                       Mean reward: -0.88
               Mean episode length: 16.10
Episode_Reward/track_lin_vel_xy_exp: 0.0047
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0199
      Episode_Reward/ang_vel_xy_l2: -0.0128
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0038
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0024
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0272
Metrics/base_velocity/error_vel_yaw: 0.0276
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184800
                    Iteration time: 0.49s
                        Total time: 37.13s
                               ETA: 108.0s

################################################################################
                      [1m Learning iteration 77/300 [0m                       

                       Computation: 4875 steps/s (collection: 0.410s, learning 0.083s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 16.0756
                       Mean reward: -0.83
               Mean episode length: 15.65
Episode_Reward/track_lin_vel_xy_exp: 0.0051
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0198
      Episode_Reward/ang_vel_xy_l2: -0.0125
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0096
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0025
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0255
Metrics/base_velocity/error_vel_yaw: 0.0276
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 187200
                    Iteration time: 0.49s
                        Total time: 37.62s
                               ETA: 107.6s

################################################################################
                      [1m Learning iteration 78/300 [0m                       

                       Computation: 4858 steps/s (collection: 0.408s, learning 0.086s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0163
                 Mean entropy loss: 16.0616
                       Mean reward: -0.85
               Mean episode length: 16.31
Episode_Reward/track_lin_vel_xy_exp: 0.0043
Episode_Reward/track_ang_vel_z_exp: 0.0031
       Episode_Reward/lin_vel_z_l2: -0.0197
      Episode_Reward/ang_vel_xy_l2: -0.0126
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0097
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0026
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0272
Metrics/base_velocity/error_vel_yaw: 0.0301
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189600
                    Iteration time: 0.49s
                        Total time: 38.11s
                               ETA: 107.1s

################################################################################
                      [1m Learning iteration 79/300 [0m                       

                       Computation: 4862 steps/s (collection: 0.411s, learning 0.083s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 16.0507
                       Mean reward: -0.82
               Mean episode length: 16.07
Episode_Reward/track_lin_vel_xy_exp: 0.0046
Episode_Reward/track_ang_vel_z_exp: 0.0032
       Episode_Reward/lin_vel_z_l2: -0.0193
      Episode_Reward/ang_vel_xy_l2: -0.0122
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0086
     Episode_Reward/action_rate_l2: -0.0038
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0027
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0287
Metrics/base_velocity/error_vel_yaw: 0.0292
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192000
                    Iteration time: 0.49s
                        Total time: 38.61s
                               ETA: 106.6s

################################################################################
                      [1m Learning iteration 80/300 [0m                       

                       Computation: 4873 steps/s (collection: 0.410s, learning 0.082s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0102
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 16.0397
                       Mean reward: -0.85
               Mean episode length: 16.31
Episode_Reward/track_lin_vel_xy_exp: 0.0043
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0199
      Episode_Reward/ang_vel_xy_l2: -0.0128
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0025
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0278
Metrics/base_velocity/error_vel_yaw: 0.0263
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 194400
                    Iteration time: 0.49s
                        Total time: 39.10s
                               ETA: 106.2s

################################################################################
                      [1m Learning iteration 81/300 [0m                       

                       Computation: 4860 steps/s (collection: 0.411s, learning 0.083s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0116
               Mean surrogate loss: -0.0170
                 Mean entropy loss: 16.0307
                       Mean reward: -0.85
               Mean episode length: 15.44
Episode_Reward/track_lin_vel_xy_exp: 0.0042
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0198
      Episode_Reward/ang_vel_xy_l2: -0.0129
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0089
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0024
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0264
Metrics/base_velocity/error_vel_yaw: 0.0256
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 196800
                    Iteration time: 0.49s
                        Total time: 39.59s
                               ETA: 105.7s

################################################################################
                      [1m Learning iteration 82/300 [0m                       

                       Computation: 4888 steps/s (collection: 0.408s, learning 0.083s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0102
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 16.0225
                       Mean reward: -0.82
               Mean episode length: 16.23
Episode_Reward/track_lin_vel_xy_exp: 0.0050
Episode_Reward/track_ang_vel_z_exp: 0.0035
       Episode_Reward/lin_vel_z_l2: -0.0191
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0027
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0267
Metrics/base_velocity/error_vel_yaw: 0.0266
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 199200
                    Iteration time: 0.49s
                        Total time: 40.08s
                               ETA: 105.3s

################################################################################
                      [1m Learning iteration 83/300 [0m                       

                       Computation: 4863 steps/s (collection: 0.411s, learning 0.083s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0161
                 Mean entropy loss: 16.0109
                       Mean reward: -0.79
               Mean episode length: 16.05
Episode_Reward/track_lin_vel_xy_exp: 0.0048
Episode_Reward/track_ang_vel_z_exp: 0.0038
       Episode_Reward/lin_vel_z_l2: -0.0189
      Episode_Reward/ang_vel_xy_l2: -0.0130
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0081
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0027
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0276
Metrics/base_velocity/error_vel_yaw: 0.0268
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 201600
                    Iteration time: 0.49s
                        Total time: 40.58s
                               ETA: 104.8s

################################################################################
                      [1m Learning iteration 84/300 [0m                       

                       Computation: 4866 steps/s (collection: 0.411s, learning 0.082s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0178
                 Mean entropy loss: 15.9968
                       Mean reward: -0.83
               Mean episode length: 15.79
Episode_Reward/track_lin_vel_xy_exp: 0.0050
Episode_Reward/track_ang_vel_z_exp: 0.0030
       Episode_Reward/lin_vel_z_l2: -0.0190
      Episode_Reward/ang_vel_xy_l2: -0.0131
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0087
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0034
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0275
Metrics/base_velocity/error_vel_yaw: 0.0322
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 204000
                    Iteration time: 0.49s
                        Total time: 41.07s
                               ETA: 104.4s

################################################################################
                      [1m Learning iteration 85/300 [0m                       

                       Computation: 4887 steps/s (collection: 0.409s, learning 0.082s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0106
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 15.9872
                       Mean reward: -0.82
               Mean episode length: 16.50
Episode_Reward/track_lin_vel_xy_exp: 0.0051
Episode_Reward/track_ang_vel_z_exp: 0.0035
       Episode_Reward/lin_vel_z_l2: -0.0182
      Episode_Reward/ang_vel_xy_l2: -0.0126
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0093
     Episode_Reward/action_rate_l2: -0.0038
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0029
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0263
Metrics/base_velocity/error_vel_yaw: 0.0290
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 206400
                    Iteration time: 0.49s
                        Total time: 41.56s
                               ETA: 103.9s

################################################################################
                      [1m Learning iteration 86/300 [0m                       

                       Computation: 4887 steps/s (collection: 0.407s, learning 0.084s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0116
               Mean surrogate loss: -0.0227
                 Mean entropy loss: 15.9612
                       Mean reward: -0.81
               Mean episode length: 16.38
Episode_Reward/track_lin_vel_xy_exp: 0.0044
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0185
      Episode_Reward/ang_vel_xy_l2: -0.0122
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0093
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0028
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0279
Metrics/base_velocity/error_vel_yaw: 0.0283
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 208800
                    Iteration time: 0.49s
                        Total time: 42.05s
                               ETA: 103.4s

################################################################################
                      [1m Learning iteration 87/300 [0m                       

                       Computation: 4921 steps/s (collection: 0.406s, learning 0.082s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0098
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 15.9516
                       Mean reward: -0.79
               Mean episode length: 16.56
Episode_Reward/track_lin_vel_xy_exp: 0.0045
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0180
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0087
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0030
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0290
Metrics/base_velocity/error_vel_yaw: 0.0280
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 211200
                    Iteration time: 0.49s
                        Total time: 42.54s
                               ETA: 103.0s

################################################################################
                      [1m Learning iteration 88/300 [0m                       

                       Computation: 4868 steps/s (collection: 0.411s, learning 0.082s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0094
               Mean surrogate loss: -0.0174
                 Mean entropy loss: 15.9369
                       Mean reward: -0.80
               Mean episode length: 15.98
Episode_Reward/track_lin_vel_xy_exp: 0.0043
Episode_Reward/track_ang_vel_z_exp: 0.0035
       Episode_Reward/lin_vel_z_l2: -0.0177
      Episode_Reward/ang_vel_xy_l2: -0.0121
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0095
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0029
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0282
Metrics/base_velocity/error_vel_yaw: 0.0287
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 213600
                    Iteration time: 0.49s
                        Total time: 43.03s
                               ETA: 102.5s

################################################################################
                      [1m Learning iteration 89/300 [0m                       

                       Computation: 4876 steps/s (collection: 0.409s, learning 0.083s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0095
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 15.9311
                       Mean reward: -0.81
               Mean episode length: 15.84
Episode_Reward/track_lin_vel_xy_exp: 0.0041
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0180
      Episode_Reward/ang_vel_xy_l2: -0.0126
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0088
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0029
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0281
Metrics/base_velocity/error_vel_yaw: 0.0267
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 216000
                    Iteration time: 0.49s
                        Total time: 43.52s
                               ETA: 102.0s

################################################################################
                      [1m Learning iteration 90/300 [0m                       

                       Computation: 4873 steps/s (collection: 0.409s, learning 0.083s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0111
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 15.9220
                       Mean reward: -0.79
               Mean episode length: 16.48
Episode_Reward/track_lin_vel_xy_exp: 0.0046
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0173
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0091
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0031
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0279
Metrics/base_velocity/error_vel_yaw: 0.0288
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 218400
                    Iteration time: 0.49s
                        Total time: 44.02s
                               ETA: 101.6s

################################################################################
                      [1m Learning iteration 91/300 [0m                       

                       Computation: 4846 steps/s (collection: 0.412s, learning 0.083s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0215
                 Mean entropy loss: 15.9023
                       Mean reward: -0.80
               Mean episode length: 16.55
Episode_Reward/track_lin_vel_xy_exp: 0.0044
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0177
      Episode_Reward/ang_vel_xy_l2: -0.0126
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0090
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0030
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0282
Metrics/base_velocity/error_vel_yaw: 0.0301
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 220800
                    Iteration time: 0.50s
                        Total time: 44.51s
                               ETA: 101.1s

################################################################################
                      [1m Learning iteration 92/300 [0m                       

                       Computation: 4849 steps/s (collection: 0.410s, learning 0.085s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0107
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 15.8915
                       Mean reward: -0.81
               Mean episode length: 16.42
Episode_Reward/track_lin_vel_xy_exp: 0.0046
Episode_Reward/track_ang_vel_z_exp: 0.0032
       Episode_Reward/lin_vel_z_l2: -0.0176
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0096
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0030
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0293
Metrics/base_velocity/error_vel_yaw: 0.0295
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 223200
                    Iteration time: 0.49s
                        Total time: 45.01s
                               ETA: 100.7s

################################################################################
                      [1m Learning iteration 93/300 [0m                       

                       Computation: 4900 steps/s (collection: 0.409s, learning 0.080s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0094
               Mean surrogate loss: -0.0164
                 Mean entropy loss: 15.8784
                       Mean reward: -0.76
               Mean episode length: 16.49
Episode_Reward/track_lin_vel_xy_exp: 0.0051
Episode_Reward/track_ang_vel_z_exp: 0.0031
       Episode_Reward/lin_vel_z_l2: -0.0169
      Episode_Reward/ang_vel_xy_l2: -0.0120
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0093
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0030
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0269
Metrics/base_velocity/error_vel_yaw: 0.0307
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 225600
                    Iteration time: 0.49s
                        Total time: 45.50s
                               ETA: 100.2s

################################################################################
                      [1m Learning iteration 94/300 [0m                       

                       Computation: 4882 steps/s (collection: 0.408s, learning 0.084s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0075
               Mean surrogate loss: -0.0253
                 Mean entropy loss: 15.8505
                       Mean reward: -0.78
               Mean episode length: 16.46
Episode_Reward/track_lin_vel_xy_exp: 0.0047
Episode_Reward/track_ang_vel_z_exp: 0.0037
       Episode_Reward/lin_vel_z_l2: -0.0169
      Episode_Reward/ang_vel_xy_l2: -0.0124
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0093
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0031
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0286
Metrics/base_velocity/error_vel_yaw: 0.0282
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 228000
                    Iteration time: 0.49s
                        Total time: 45.99s
                               ETA: 99.7s

################################################################################
                      [1m Learning iteration 95/300 [0m                       

                       Computation: 4860 steps/s (collection: 0.412s, learning 0.082s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0086
               Mean surrogate loss: -0.0191
                 Mean entropy loss: 15.8309
                       Mean reward: -0.78
               Mean episode length: 16.44
Episode_Reward/track_lin_vel_xy_exp: 0.0051
Episode_Reward/track_ang_vel_z_exp: 0.0037
       Episode_Reward/lin_vel_z_l2: -0.0173
      Episode_Reward/ang_vel_xy_l2: -0.0122
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0090
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0031
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0272
Metrics/base_velocity/error_vel_yaw: 0.0280
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 230400
                    Iteration time: 0.49s
                        Total time: 46.48s
                               ETA: 99.3s

################################################################################
                      [1m Learning iteration 96/300 [0m                       

                       Computation: 4879 steps/s (collection: 0.409s, learning 0.083s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0102
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 15.8235
                       Mean reward: -0.81
               Mean episode length: 16.71
Episode_Reward/track_lin_vel_xy_exp: 0.0044
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0099
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0032
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0301
Metrics/base_velocity/error_vel_yaw: 0.0294
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 232800
                    Iteration time: 0.49s
                        Total time: 46.97s
                               ETA: 98.8s

################################################################################
                      [1m Learning iteration 97/300 [0m                       

                       Computation: 4876 steps/s (collection: 0.410s, learning 0.082s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0096
               Mean surrogate loss: -0.0161
                 Mean entropy loss: 15.8157
                       Mean reward: -0.78
               Mean episode length: 15.88
Episode_Reward/track_lin_vel_xy_exp: 0.0047
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0170
      Episode_Reward/ang_vel_xy_l2: -0.0120
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0098
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0028
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0271
Metrics/base_velocity/error_vel_yaw: 0.0269
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 235200
                    Iteration time: 0.49s
                        Total time: 47.47s
                               ETA: 98.3s

################################################################################
                      [1m Learning iteration 98/300 [0m                       

                       Computation: 4869 steps/s (collection: 0.409s, learning 0.084s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 15.8070
                       Mean reward: -0.77
               Mean episode length: 16.59
Episode_Reward/track_lin_vel_xy_exp: 0.0046
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0162
      Episode_Reward/ang_vel_xy_l2: -0.0123
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0091
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0032
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0285
Metrics/base_velocity/error_vel_yaw: 0.0312
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 237600
                    Iteration time: 0.49s
                        Total time: 47.96s
                               ETA: 97.9s

################################################################################
                      [1m Learning iteration 99/300 [0m                       

                       Computation: 4860 steps/s (collection: 0.412s, learning 0.082s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0073
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 15.7981
                       Mean reward: -0.75
               Mean episode length: 16.18
Episode_Reward/track_lin_vel_xy_exp: 0.0051
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0170
      Episode_Reward/ang_vel_xy_l2: -0.0120
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0084
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0031
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0273
Metrics/base_velocity/error_vel_yaw: 0.0279
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 240000
                    Iteration time: 0.49s
                        Total time: 48.45s
                               ETA: 97.4s

################################################################################
                      [1m Learning iteration 100/300 [0m                      

                       Computation: 4885 steps/s (collection: 0.408s, learning 0.083s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 15.7878
                       Mean reward: -0.78
               Mean episode length: 16.57
Episode_Reward/track_lin_vel_xy_exp: 0.0047
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0125
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0098
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0031
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0281
Metrics/base_velocity/error_vel_yaw: 0.0294
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 242400
                    Iteration time: 0.49s
                        Total time: 48.94s
                               ETA: 96.9s

################################################################################
                      [1m Learning iteration 101/300 [0m                      

                       Computation: 4868 steps/s (collection: 0.411s, learning 0.082s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0073
               Mean surrogate loss: -0.0175
                 Mean entropy loss: 15.7736
                       Mean reward: -0.78
               Mean episode length: 16.22
Episode_Reward/track_lin_vel_xy_exp: 0.0041
Episode_Reward/track_ang_vel_z_exp: 0.0035
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0117
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0095
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0032
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0297
Metrics/base_velocity/error_vel_yaw: 0.0287
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 244800
                    Iteration time: 0.49s
                        Total time: 49.44s
                               ETA: 96.5s

################################################################################
                      [1m Learning iteration 102/300 [0m                      

                       Computation: 4887 steps/s (collection: 0.407s, learning 0.084s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 15.7679
                       Mean reward: -0.75
               Mean episode length: 16.64
Episode_Reward/track_lin_vel_xy_exp: 0.0041
Episode_Reward/track_ang_vel_z_exp: 0.0038
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0118
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0093
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0032
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0307
Metrics/base_velocity/error_vel_yaw: 0.0277
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 247200
                    Iteration time: 0.49s
                        Total time: 49.93s
                               ETA: 96.0s

################################################################################
                      [1m Learning iteration 103/300 [0m                      

                       Computation: 4849 steps/s (collection: 0.412s, learning 0.082s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0168
                 Mean entropy loss: 15.7590
                       Mean reward: -0.77
               Mean episode length: 16.12
Episode_Reward/track_lin_vel_xy_exp: 0.0043
Episode_Reward/track_ang_vel_z_exp: 0.0038
       Episode_Reward/lin_vel_z_l2: -0.0161
      Episode_Reward/ang_vel_xy_l2: -0.0121
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0093
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0031
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0283
Metrics/base_velocity/error_vel_yaw: 0.0262
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 249600
                    Iteration time: 0.49s
                        Total time: 50.42s
                               ETA: 95.5s

################################################################################
                      [1m Learning iteration 104/300 [0m                      

                       Computation: 4865 steps/s (collection: 0.408s, learning 0.085s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0211
                 Mean entropy loss: 15.7376
                       Mean reward: -0.77
               Mean episode length: 16.68
Episode_Reward/track_lin_vel_xy_exp: 0.0044
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0160
      Episode_Reward/ang_vel_xy_l2: -0.0123
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0096
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0032
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0292
Metrics/base_velocity/error_vel_yaw: 0.0285
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 252000
                    Iteration time: 0.49s
                        Total time: 50.92s
                               ETA: 95.0s

################################################################################
                      [1m Learning iteration 105/300 [0m                      

                       Computation: 4856 steps/s (collection: 0.411s, learning 0.083s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0076
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 15.7277
                       Mean reward: -0.75
               Mean episode length: 16.66
Episode_Reward/track_lin_vel_xy_exp: 0.0039
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0124
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0034
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0300
Metrics/base_velocity/error_vel_yaw: 0.0296
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 254400
                    Iteration time: 0.49s
                        Total time: 51.41s
                               ETA: 94.6s

################################################################################
                      [1m Learning iteration 106/300 [0m                      

                       Computation: 4887 steps/s (collection: 0.409s, learning 0.083s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 15.7131
                       Mean reward: -0.76
               Mean episode length: 17.03
Episode_Reward/track_lin_vel_xy_exp: 0.0046
Episode_Reward/track_ang_vel_z_exp: 0.0038
       Episode_Reward/lin_vel_z_l2: -0.0154
      Episode_Reward/ang_vel_xy_l2: -0.0125
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0092
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0039
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0289
Metrics/base_velocity/error_vel_yaw: 0.0281
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 256800
                    Iteration time: 0.49s
                        Total time: 51.90s
                               ETA: 94.1s

################################################################################
                      [1m Learning iteration 107/300 [0m                      

                       Computation: 4882 steps/s (collection: 0.409s, learning 0.083s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0084
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 15.7079
                       Mean reward: -0.75
               Mean episode length: 17.00
Episode_Reward/track_lin_vel_xy_exp: 0.0046
Episode_Reward/track_ang_vel_z_exp: 0.0037
       Episode_Reward/lin_vel_z_l2: -0.0152
      Episode_Reward/ang_vel_xy_l2: -0.0120
     Episode_Reward/dof_torques_l2: -0.0015
         Episode_Reward/dof_acc_l2: -0.0097
     Episode_Reward/action_rate_l2: -0.0037
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0035
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0292
Metrics/base_velocity/error_vel_yaw: 0.0287
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 259200
                    Iteration time: 0.49s
                        Total time: 52.39s
                               ETA: 93.6s

################################################################################
                      [1m Learning iteration 108/300 [0m                      

                       Computation: 4872 steps/s (collection: 0.409s, learning 0.083s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 15.7012
                       Mean reward: -0.74
               Mean episode length: 16.54
Episode_Reward/track_lin_vel_xy_exp: 0.0044
Episode_Reward/track_ang_vel_z_exp: 0.0037
       Episode_Reward/lin_vel_z_l2: -0.0155
      Episode_Reward/ang_vel_xy_l2: -0.0123
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0092
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0037
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0288
Metrics/base_velocity/error_vel_yaw: 0.0265
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 261600
                    Iteration time: 0.49s
                        Total time: 52.89s
                               ETA: 93.2s

################################################################################
                      [1m Learning iteration 109/300 [0m                      

                       Computation: 4863 steps/s (collection: 0.409s, learning 0.085s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0102
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 15.6904
                       Mean reward: -0.76
               Mean episode length: 16.28
Episode_Reward/track_lin_vel_xy_exp: 0.0042
Episode_Reward/track_ang_vel_z_exp: 0.0033
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0121
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0095
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0032
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0285
Metrics/base_velocity/error_vel_yaw: 0.0272
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 264000
                    Iteration time: 0.49s
                        Total time: 53.38s
                               ETA: 92.7s

################################################################################
                      [1m Learning iteration 110/300 [0m                      

                       Computation: 4874 steps/s (collection: 0.408s, learning 0.084s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0181
                 Mean entropy loss: 15.6734
                       Mean reward: -0.71
               Mean episode length: 16.51
Episode_Reward/track_lin_vel_xy_exp: 0.0054
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0120
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0087
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0033
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0269
Metrics/base_velocity/error_vel_yaw: 0.0256
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 266400
                    Iteration time: 0.49s
                        Total time: 53.87s
                               ETA: 92.2s

################################################################################
                      [1m Learning iteration 111/300 [0m                      

                       Computation: 4859 steps/s (collection: 0.410s, learning 0.084s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 15.6534
                       Mean reward: -0.74
               Mean episode length: 16.10
Episode_Reward/track_lin_vel_xy_exp: 0.0046
Episode_Reward/track_ang_vel_z_exp: 0.0035
       Episode_Reward/lin_vel_z_l2: -0.0149
      Episode_Reward/ang_vel_xy_l2: -0.0118
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0090
     Episode_Reward/action_rate_l2: -0.0034
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0035
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0276
Metrics/base_velocity/error_vel_yaw: 0.0279
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 268800
                    Iteration time: 0.49s
                        Total time: 54.37s
                               ETA: 91.7s

################################################################################
                      [1m Learning iteration 112/300 [0m                      

                       Computation: 4892 steps/s (collection: 0.407s, learning 0.083s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0076
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 15.6472
                       Mean reward: -0.71
               Mean episode length: 16.77
Episode_Reward/track_lin_vel_xy_exp: 0.0046
Episode_Reward/track_ang_vel_z_exp: 0.0040
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0123
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0039
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0295
Metrics/base_velocity/error_vel_yaw: 0.0273
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 271200
                    Iteration time: 0.49s
                        Total time: 54.86s
                               ETA: 91.3s

################################################################################
                      [1m Learning iteration 113/300 [0m                      

                       Computation: 4847 steps/s (collection: 0.409s, learning 0.086s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 15.6401
                       Mean reward: -0.73
               Mean episode length: 16.81
Episode_Reward/track_lin_vel_xy_exp: 0.0047
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0117
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0087
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0037
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0298
Metrics/base_velocity/error_vel_yaw: 0.0297
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 273600
                    Iteration time: 0.50s
                        Total time: 55.35s
                               ETA: 90.8s

################################################################################
                      [1m Learning iteration 114/300 [0m                      

                       Computation: 4903 steps/s (collection: 0.407s, learning 0.083s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0085
               Mean surrogate loss: -0.0208
                 Mean entropy loss: 15.6175
                       Mean reward: -0.73
               Mean episode length: 16.30
Episode_Reward/track_lin_vel_xy_exp: 0.0048
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0117
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0095
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0036
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0284
Metrics/base_velocity/error_vel_yaw: 0.0276
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 276000
                    Iteration time: 0.49s
                        Total time: 55.84s
                               ETA: 90.3s

################################################################################
                      [1m Learning iteration 115/300 [0m                      

                       Computation: 4843 steps/s (collection: 0.412s, learning 0.084s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0101
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 15.6062
                       Mean reward: -0.71
               Mean episode length: 16.66
Episode_Reward/track_lin_vel_xy_exp: 0.0049
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0114
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0088
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0036
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0284
Metrics/base_velocity/error_vel_yaw: 0.0281
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 278400
                    Iteration time: 0.50s
                        Total time: 56.34s
                               ETA: 89.8s

################################################################################
                      [1m Learning iteration 116/300 [0m                      

                       Computation: 4871 steps/s (collection: 0.408s, learning 0.084s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 15.5973
                       Mean reward: -0.71
               Mean episode length: 16.53
Episode_Reward/track_lin_vel_xy_exp: 0.0048
Episode_Reward/track_ang_vel_z_exp: 0.0037
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0117
     Episode_Reward/dof_torques_l2: -0.0013
         Episode_Reward/dof_acc_l2: -0.0086
     Episode_Reward/action_rate_l2: -0.0034
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0035
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0275
Metrics/base_velocity/error_vel_yaw: 0.0261
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 280800
                    Iteration time: 0.49s
                        Total time: 56.83s
                               ETA: 89.4s

################################################################################
                      [1m Learning iteration 117/300 [0m                      

                       Computation: 4864 steps/s (collection: 0.409s, learning 0.085s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0198
                 Mean entropy loss: 15.5768
                       Mean reward: -0.74
               Mean episode length: 17.13
Episode_Reward/track_lin_vel_xy_exp: 0.0041
Episode_Reward/track_ang_vel_z_exp: 0.0034
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0115
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0095
     Episode_Reward/action_rate_l2: -0.0036
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0036
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0307
Metrics/base_velocity/error_vel_yaw: 0.0284
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 283200
                    Iteration time: 0.49s
                        Total time: 57.32s
                               ETA: 88.9s

################################################################################
                      [1m Learning iteration 118/300 [0m                      

                       Computation: 4900 steps/s (collection: 0.406s, learning 0.084s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0066
               Mean surrogate loss: -0.0179
                 Mean entropy loss: 15.5593
                       Mean reward: -0.70
               Mean episode length: 16.54
Episode_Reward/track_lin_vel_xy_exp: 0.0045
Episode_Reward/track_ang_vel_z_exp: 0.0038
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0108
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0089
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0034
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0285
Metrics/base_velocity/error_vel_yaw: 0.0267
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 285600
                    Iteration time: 0.49s
                        Total time: 57.81s
                               ETA: 88.4s

################################################################################
                      [1m Learning iteration 119/300 [0m                      

                       Computation: 4855 steps/s (collection: 0.409s, learning 0.085s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0098
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 15.5521
                       Mean reward: -0.72
               Mean episode length: 16.54
Episode_Reward/track_lin_vel_xy_exp: 0.0043
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0111
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0094
     Episode_Reward/action_rate_l2: -0.0034
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: -0.0037
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0297
Metrics/base_velocity/error_vel_yaw: 0.0288
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 288000
                    Iteration time: 0.49s
                        Total time: 58.31s
                               ETA: 87.9s

################################################################################
                      [1m Learning iteration 120/300 [0m                      

                       Computation: 4864 steps/s (collection: 0.407s, learning 0.086s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0070
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 15.5449
                       Mean reward: -0.71
               Mean episode length: 16.81
Episode_Reward/track_lin_vel_xy_exp: 0.0047
Episode_Reward/track_ang_vel_z_exp: 0.0036
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0113
     Episode_Reward/dof_torques_l2: -0.0014
         Episode_Reward/dof_acc_l2: -0.0091
     Episode_Reward/action_rate_l2: -0.0035
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: -0.0036
     Episode_Reward/dof_pos_limits: 0.0000
Metrics/base_velocity/error_vel_xy: 0.0287
Metrics/base_velocity/error_vel_yaw: 0.0278
      Episode_Termination/time_out: 0.0000
  Episode_Termination/base_contact: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 290400
                    Iteration time: 0.49s
                        Total time: 58.80s
                               ETA: 87.5s

